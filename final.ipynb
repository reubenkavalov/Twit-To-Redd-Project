{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Importing Required Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import praw\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from keras.preprocessing import text\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import History, EarlyStopping, ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring User Likes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquired from Twitter using API key and API secret key ###\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAImyAwEAAAAAyWB9oAfMqhJG66%2BeJHuUEi5wsXM%3DKOzTDPgjqoXjzERO1JmoAJfVIqjnaPdD7wUhayNjESzeytty2J' # Bearer Token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liked_tweets(handle):\n",
    "    '''\n",
    "    Takes Twitter handle and extracts and parses liked tweets.\n",
    "    \n",
    "    '''\n",
    "    all_tweets = []\n",
    "    url = 'https://api.twitter.com/1.1/favorites/list.json?screen_name={}'.format(handle)\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer {}'.format(bearer_token),\n",
    "            }\n",
    "    first_url_params = {\n",
    "        'count': 200\n",
    "            }\n",
    "    first_response = requests.get(url, headers=headers, params=first_url_params)\n",
    "    print(first_response)\n",
    "    first_initial_split = first_response.text.split(',\"text\":\"')\n",
    "    first_t = len(first_initial_split)    \n",
    "    for i in range(1,first_t):\n",
    "        all_tweets.append(first_initial_split[i].split(',\"truncated\":')[0])\n",
    "        \n",
    "    ### Regex to extract the id of the last tweet to use as pagination ###      \n",
    "    regsearch = re.search('(?<=(\\d\",\"id\":))[\\d]*', first_initial_split[first_t-2])\n",
    "    tweet_id = int(regsearch.group(0))\n",
    "    ###\n",
    "    print(tweet_id)\n",
    "    \n",
    "    ### Looping process to manually paginate through tweets ###\n",
    "    page = 0\n",
    "    while page < 15:\n",
    "        url1 = 'https://api.twitter.com/1.1/favorites/list.json?screen_name={}&count=200&max_id={}'.format(handle,tweet_id-1)\n",
    "        response = requests.get(url1, headers=headers)\n",
    "        print(response)\n",
    "        initial_split = response.text.split(',\"text\":\"')\n",
    "        t = len(initial_split)\n",
    "        print(t)\n",
    "        for i in range(1,t):\n",
    "            all_tweets.append(initial_split[i].split(',\"truncated\":')[0])\n",
    " \n",
    "        ### Regex to extract the id of the last tweet to use as pagination ###      \n",
    "        \n",
    "        try:\n",
    "            regsearch = re.search('(?<=(\\d\",\"id\":))[\\d]*', initial_split[t-2])\n",
    "            tweet_id = int(regsearch.group(0))\n",
    "            print(tweet_id)\n",
    "        except: break\n",
    "        ###\n",
    "        page +=1\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Top Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.reddit.com/r/TheoryOfReddit/comments/1f7hqc/the_200_most_active_subreddits_categorized_by/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = driver.find_elements_by_class_name('_1qeIAgB0cPwnLhDF9XSiJM')\n",
    "subreddits = []\n",
    "for i in subs[3:]:\n",
    "    if \"r/\" in i.text:\n",
    "        subreddits.append(i.text.split(\" - \")[1])\n",
    "    else:\n",
    "        continue\n",
    "### THIS CELL THROWS AN ERROR -- IGNORE IT -- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_subs = []\n",
    "for sub in subreddits:\n",
    "    clean_subs.append(sub.split('r/')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PRAW (Python Reddit API Wrapper) to get top posts in top subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='MU9yce7zmiOKwA',\n",
    "                     client_secret='DRV5fO7p-Tn7VzxmMH8mnRwM_hE',\n",
    "                     user_agent='my user agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = {}\n",
    "for sub in tqdm(clean_subs):\n",
    "    all_submissions[sub] = []\n",
    "    for submission in reddit.subreddit(sub).top(limit=1000):\n",
    "        all_submissions[sub].append(submission.title)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at examples of pre-processing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "subr_text_tokens_raw = nltk.regexp_tokenize(\" \".join(all_submissions['creepy']), pattern)\n",
    "subr_text_tokens = [word.lower() for word in subr_text_tokens_raw]\n",
    "subr_text_tokens = [lemmatizer.lemmatize(i) for i in subr_text_tokens]\n",
    "subr_text_freqdist = FreqDist(subr_text_tokens)\n",
    "subr_text_freqdist.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_text_stopped_freqdist = FreqDist(subr_text_stopped)\n",
    "subr_text_stopped_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_word_count = sum(subr_text_stopped_freqdist.values())\n",
    "subr_text_top_50 = subr_text_stopped_freqdist.most_common(50)\n",
    "print(\"Word\\t\\t\\tNormalized Frequency\")\n",
    "for word in subr_text_top_50:\n",
    "    normalized_frequency = word[1] / total_word_count\n",
    "    print(\"{} \\t\\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "subr_text_finder = BigramCollocationFinder.from_words(subr_text_stopped)\n",
    "subr_text_finder.apply_freq_filter(3)\n",
    "\n",
    "subr_text_scored = subr_text_finder.score_ngrams(bigram_measures.pmi)\n",
    "subr_text_scored[:20]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating subreddits into respective categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text_df = pd.DataFrame()\n",
    "\n",
    "label_text_df['target'] = ['Discussion and Stories','Emotional Reaction Fuel','Entertainment - Gaming', \n",
    "                         'Entertainment - Television', 'Entertainment - Other (Movies/Music/Franchies/Misc)',\n",
    "                         'Humor','Images, Gifs, and Videos','Learning and Thinking','Lifestyle and Help',\n",
    "                         'News and Issues', 'Places', 'Race, Gender, and Identity', 'Sports','Technology']\n",
    "categorized_posts = dict((i,\"\")for i in label_text_df.target)\n",
    "for i in clean_subs:\n",
    "    if i ==\"AskReddit\" or i=='IAmA' or i ==\"bestof\" or i=='fatpeoplestories' or i =='pettyrevenge' or i ==\"TalesFromRetail\" or i=='DoesAnybodyElse' or i =='CrazyIdeas':\n",
    "        categorized_posts['Discussion and Stories'] = [categorized_posts['Discussion and Stories'],all_submissions[i]]\n",
    "    elif i ==\"WTF\" or i=='aww' or i ==\"cringepics\" or i=='JusticePorn' or i =='MorbidReality' or i ==\"rage\" or i=='mildlyinfuriating' or i =='creepy' or i=='creepyPMs' or i ==\"nosleep\" or i=='nostalgia':\n",
    "        categorized_posts['Emotional Reaction Fuel'] = [categorized_posts['Emotional Reaction Fuel'],all_submissions[i]]\n",
    "    elif i ==\"gaming\" or i=='leagueoflegends' or i ==\"pokemon\" or i=='Minecraft' or i =='starcraft' or i ==\"Games\" or i=='DotA2' or i =='skyrim' or i=='tf2' or i ==\"magicTCG\" or i=='wow' or i ==\"KerbalSpaceProgram\" or i=='mindcrack' or i =='Fallout' or i=='roosterteeth' or i ==\"Planetside\" or i=='gamegrumps' or i ==\"battlefield3\" or i=='zelda' or i =='darksouls' or i=='masseffect':\n",
    "        categorized_posts['Entertainment - Gaming'] = [categorized_posts['Entertainment - Gaming'],all_submissions[i]]\n",
    "    elif i ==\"arresteddevelopment\" or i=='gameofthrones' or i ==\"doctorwho\" or i=='mylittlepony' or i =='community' or i ==\"breakingbad\" or i=='adventuretime' or i =='startrek' or i=='TheSimpsons' or i ==\"futurama\" or i=='HIMYM' or i=='DunderMifflin' or i ==\"thewalkingdead\":\n",
    "        categorized_posts['Entertainment - Television'] = [categorized_posts['Entertainment - Television'],all_submissions[i]]\n",
    "    elif i ==\"Music\" or i=='movies' or i ==\"harrypotter\" or i=='StarWars' or i =='DaftPunk' or i ==\"hiphopheads\" or i=='anime' or i =='comicbooks' or i=='geek' or i ==\"batman\" or i=='TheLastAirbender' or i=='Naruto' or i ==\"FanTheories\":\n",
    "        categorized_posts['Entertainment - Other (Movies/Music/Franchies/Misc)'] = [categorized_posts['Entertainment - Other (Movies/Music/Franchies/Misc)'],all_submissions[i]]\n",
    "    elif i ==\"funny\" or i=='AdviceAnimals' or i ==\"fffffffuuuuuuuuuuuu\" or i=='4chan' or i =='ImGoingToHellForThis' or i ==\"firstworldanarchists\" or i=='circlejerk' or i =='MURICA' or i=='facepalm' or i ==\"Jokes\" or i=='wheredidthesodago' or i=='polandball' or i ==\"TrollXChromosomes\" or i ==\"comics\" or i=='nottheonion' or i=='britishproblems' or i ==\"TumblrInAction\" or i ==\"onetruegod\":\n",
    "        categorized_posts['Humor'] = [categorized_posts['Humor'],all_submissions[i]]\n",
    "    elif i ==\"pics\" or i=='videos' or i ==\"gifs\" or i=='reactiongifs' or i =='mildlyinteresting' or i ==\"woahdude\" or i=='FiftyFifty' or i =='FoodPorn' or i=='HistoryPorn' or i ==\"wallpapers\" or i=='youtubehaiku' or i=='Unexpected' or i ==\"photoshopbattles\" or i ==\"AnimalsBeingJerks\" or i=='cosplay' or i=='EarthPorn' or i ==\"QuotesPorn\" or i ==\"awwnime\" or i ==\"AbandonedPorn\" or i=='carporn' or i ==\"PerfectTiming\" or i=='OldSchoolCool' or i =='RoomPorn' or i ==\"woahdude\" or i=='Pareidolia' or i =='MapPorn' or i=='tumblr' or i ==\"techsupportgore\" or i=='PrettyGirls' or i=='itookapicture':\n",
    "        categorized_posts['Images, Gifs, and Videos'] = [categorized_posts['Images, Gifs, and Videos'],all_submissions[i]]\n",
    "    elif i ==\"todayilearned\" or i=='science' or i ==\"askscience\" or i=='space' or i =='AskHistorians' or i ==\"YouShouldKnow\" or i=='explainlikeimfive':\n",
    "        categorized_posts['Learning and Thinking'] = [categorized_posts['Learning and Thinking'],all_submissions[i]]\n",
    "    elif i ==\"trees\" or i=='MakeupAddiction' or i ==\"cats\" or i=='LifeProTips' or i =='RedditLaqueristas' or i ==\"Random_Acts_Of_Amazon\" or i=='food' or i =='guns' or i=='tattoos' or i ==\"corgi\" or i=='teenagers' or i =='GetMotivated' or i=='motorcycles' or i ==\"sex\" or i=='progresspics' or i =='DIY' or i=='bicycling' or i ==\"Fitness\" or i=='lifehacks' or i =='longboarding' or i=='Frugal' or i ==\"drunk\" or i=='Art' or i=='loseit' or i =='Military':\n",
    "        categorized_posts['Lifestyle and Help'] = [categorized_posts['Lifestyle and Help'],all_submissions[i]]\n",
    "    elif i ==\"politics\" or i=='worldnews' or i ==\"news\" or i=='conspiracy' or i =='Libertarian' or i ==\"TrueReddit\" or i=='Conservative' or i=='offbeat':\n",
    "        categorized_posts['News and Issues'] = [categorized_posts['News and Issues'],all_submissions[i]]\n",
    "    elif i ==\"canada\" or i=='toronto' or i ==\"australia\" or i=='unitedkingdom':\n",
    "        categorized_posts['Places'] = [categorized_posts['Places'],all_submissions[i]]\n",
    "    elif i ==\"atheism\" or i=='TwoXChromosomes' or i ==\"MensRights\" or i=='gaybros' or i =='lgbt':\n",
    "        categorized_posts['Race, Gender, and Identity'] = [categorized_posts['Race, Gender, and Identity'],all_submissions[i]]\n",
    "    elif i ==\"nba\" or i=='soccer' or i ==\"hockey\" or i=='nfl' or i =='formula1' or i ==\"baseball\" or i=='MMA' or i=='SquaredCircle':\n",
    "        categorized_posts['Sports'] = [categorized_posts['Sports'],all_submissions[i]]\n",
    "    elif i ==\"technology\" or i=='Android' or i ==\"Bitcoin\" or i=='programming' or i =='apple':\n",
    "        categorized_posts['Technology'] = [categorized_posts['Technology'],all_submissions[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataframe of posts and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(container):\n",
    "    for i in container:\n",
    "        if isinstance(i, (list,tuple)):\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i\n",
    "posts_by_category = dict((i,\"\")for i in label_text_df.target)\n",
    "for i in label_text_df.target:\n",
    "    posts_by_category[i] = list(flatten(categorized_posts[i]))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = []\n",
    "for i in tqdm(label_text_df.target):\n",
    "    for post in posts_by_category[i]:\n",
    "        post_list = []\n",
    "        post_list.append(i)\n",
    "        post_list.append(post)\n",
    "        master_list.append(post_list)\n",
    "        \n",
    "#     posts_by_category[i] = \" \".join(posts_by_category[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.DataFrame(master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.to_csv('reddit_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('reddit_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.drop(columns=[\"Unnamed: 0\"])\n",
    "reddit_df.columns = reddit_df.columns.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GloVes multi-dimensional word vectorizer to build baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = reddit_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reddit_df[1].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocabulary = set(word for headline in data for word in headline)\n",
    "print(\"There are {} unique tokens in the dataset.\".format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('glove.6B.300d.txt', 'rb') as f:            ## I used the 100-dimensional file to vectorize for the building\n",
    "    for line in f:                                    # of the model, change to 300-dimensional for final model\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])\n",
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in tqdm(models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial results seem low, but, with 14 categories, random guessing would achieve an accuracy of 7.1%. Let's try a Deep Neural Network with and Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values\n",
    "reddit_df['splitted'] = reddit_df[1].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "reddit_df['text_lemmatized'] = reddit_df.splitted.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reddit_df[1] = reddit_df.text_lemmatized.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(reddit_df[1]))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(reddit_df[1])\n",
    "X_t = pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################\n",
    "# #   CURRENT BEST MODEL  #  --- Epoch 1: 15.6% accurate\n",
    "# #########################  --- Epoch 50: 49.4%\n",
    "\n",
    "#embedding_size = 300\n",
    "# input_ = Input(shape=(50,))\n",
    "# x = Embedding(20000, embedding_size)(input_)\n",
    "# x = Dense(750, activation='relu', kernel_regularizer=regularizers.l2(0.2))(x)\n",
    "\n",
    "# x = LSTM(25, return_sequences=True)(x)\n",
    "# x = GlobalMaxPool1D()(x)\n",
    "# x = Dense(500, activation='relu')(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "# # There are 14 different possible classes, so we use 14 neurons in our output layer\n",
    "# x = Dense(14, activation='softmax')(x)\n",
    "\n",
    "# model = Model(inputs=input_, outputs=x)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "# X_train,X_val, y_train,y_val = train_test_split(X_t,y,test_size=0.1,random_state=1)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network initially kept having a val_accuracy of 0... because the data is sitting in order and wasn't shuffled! (the \"test_size\" is just the very last bit (0.2) of the data, it doesn't choose randomly... fixed with train test split, which shuffles by default!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with stopwords removed from post titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df_stopped = pd.DataFrame()\n",
    "reddit_df_stopped[0] = reddit_df[0]\n",
    "reddit_df_stopped[1] = reddit_df[1].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(list(reddit_df_stopped[1]))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(reddit_df_stopped[1])\n",
    "X_t = pad_sequences(list_tokenized_headlines, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "input_ = Input(shape=(50,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = Dense(750, activation='relu', kernel_regularizer=regularizers.l2(0.2))(x)\n",
    "\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(500, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "# There are 14 different possible classes, so we use 14 neurons in our output layer\n",
    "x = Dense(14, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "X_train,X_val, y_train,y_val = train_test_split(X_t,y,test_size=0.3,random_state=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=500, callbacks=[earlyStopping,mcp_save], shuffle=True, validation_data=(X_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_val,y_val) ###  Quick sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating probability of what category you like based on tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model.json', 'r')                        ### Loading my best model\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RedditCategoryPredictor():\n",
    "    twitter_handle = input(\"What is your Twitter Handle?\")\n",
    "    likes = get_liked_tweets(twitter_handle)\n",
    "    print(len(set(likes)))\n",
    "    tweets_stopped = []\n",
    "    for sentence in set(likes):\n",
    "        tweets_stopped.append([word for word in sentence.split() if word not in stopwords and \"\\\\\" not in word and \"@\" not in word and \"#\" not in word and len(word) > 2 and \"$\" not in word and \"---\" not in word])\n",
    "    tweets_stopped_and_lemmatized = []\n",
    "    for sentence in tweets_stopped:\n",
    "        tweets_stopped_and_lemmatized.append([lemmatizer.lemmatize(word) for word in sentence])\n",
    "    print(tweets_stopped_and_lemmatized)\n",
    "    \n",
    "                                            \n",
    "    tokenizer1 = text.Tokenizer(num_words=30000)\n",
    "    tokenizer1.fit_on_texts(tweets_stopped_and_lemmatized)\n",
    "    list_tokenized_tweets = tokenizer.texts_to_sequences(tweets_stopped_and_lemmatized)\n",
    "    X_t1 = pad_sequences(list_tokenized_tweets, maxlen=50)\n",
    "    result_probabilities = model.predict(X_t1)\n",
    "    result = result_probabilities.argmax(axis=1)\n",
    "    print(result_probabilities.shape)\n",
    "    unique_elements, counts_elements = np.unique(result, return_counts=True)\n",
    "    print(unique_elements)\n",
    "    print(counts_elements)\n",
    "    winning_category = list(counts_elements).index(max(counts_elements))\n",
    "    suggested_cat = None\n",
    "    if winning_category == 0:\n",
    "        suggested_cat = \"Discussion and Stories\"\n",
    "    elif winning_category == 1:\n",
    "        suggested_cat = \"Emotional Reaction Fuel\"\n",
    "    elif winning_category == 2:\n",
    "        suggested_cat = \"Entertainment - Gaming\"\n",
    "    elif winning_category == 3:\n",
    "        suggested_cat = \"Entertainment - Other (Movies/Music/Franchies/Misc)\"\n",
    "    elif winning_category == 4:\n",
    "        suggested_cat = \"Entertainment - Television\"\n",
    "    elif winning_category == 5:\n",
    "        suggested_cat = \"Humor\"\n",
    "    elif winning_category == 6:\n",
    "        suggested_cat = \"Images, Gifs, and Videos\"\n",
    "    elif winning_category == 7:\n",
    "        suggested_cat = \"Learning and Thinking\"\n",
    "    elif winning_category == 8:\n",
    "        suggested_cat = \"Lifestyle and Help\"\n",
    "    elif winning_category == 9:\n",
    "        suggested_cat = \"News and Issues\"\n",
    "    elif winning_category == 10:\n",
    "        suggested_cat = \"Places\"\n",
    "    elif winning_category == 11:\n",
    "        suggested_cat = \"Race, Gender, and Identity\"\n",
    "    elif winning_category == 12:\n",
    "        suggested_cat = \"Sports\"\n",
    "    elif winning_category == 13:\n",
    "        suggested_cat = \"Technology\"\n",
    "    return suggested_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "techcrunch = RedditCategoryPredictor()\n",
    "techcrunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df_stopped[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Related Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://anvaka.github.io/redsim/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_subreddits = {}\n",
    "for i in clean_subs:\n",
    "    searchbox = driver.find_element_by_id('search-input')\n",
    "    searchbox.send_keys(i)\n",
    "    driver.find_element_by_class_name('btn').click()\n",
    "    result_subs = driver.find_elements_by_xpath(\"//div[contains(@ng-repeat, 'sub in to')]\")\n",
    "    for s in result_subs:\n",
    "        related_subreddits.setdefault(i,[]).append(s.text) \n",
    "    driver.find_element_by_id('search-input').clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(related_subreddits.values())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_related_posts = {}\n",
    "for related_subs in tqdm(list(related_subreddits.values())):\n",
    "    related_sub_posts = []\n",
    "    for sub in tqdm(related_subs):\n",
    "        try:\n",
    "            for submission in reddit.subreddit(sub).top(limit=1000):\n",
    "                related_sub_posts.append(submission.title)\n",
    "            related_sub_posts.append(sub)\n",
    "        except:\n",
    "            continue\n",
    "    all_related_posts.append(related_sub_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_related_posts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_related_posts[0][8000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    combined_titles = \"\"\n",
    "    for title in text:\n",
    "        combined_titles += title + \" \"\n",
    "    return word_tokenize(combined_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize(text, vocab=None):\n",
    "    if vocab:\n",
    "        unique_words = vocab\n",
    "    else:\n",
    "        unique_words = list(set(text))\n",
    "    \n",
    "    word_dict = {i:0 for i in unique_words}\n",
    "    \n",
    "    for word in text:\n",
    "        word_dict[word] += 1\n",
    "    \n",
    "    return word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in clean_subs:\n",
    "    tokenized_words = tokenize(all_submissions[sub])\n",
    "    counted_words = count_vectorize(tokenized_words)\n",
    "    print(counted_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(BoW_dict):\n",
    "    total_word_count = sum(BoW_dict.values())\n",
    "    for ind, val in BoW_dict.items():\n",
    "        BoW_dict[ind] = val/ total_word_count\n",
    "    \n",
    "    return BoW_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(list_of_dicts):\n",
    "    vocab_set = set()\n",
    "    # Iterate through list of dfs and add index to vocab_set\n",
    "    for d in list_of_dicts:\n",
    "        for word in d.keys():\n",
    "            vocab_set.add(word)\n",
    "    \n",
    "    # Once vocab set is complete, create an empty dictionary with a key for each word and value of 0.\n",
    "    full_vocab_dict = {i:0 for i in vocab_set}\n",
    "    \n",
    "    # Loop through each word in full_vocab_dict\n",
    "    for word, val in full_vocab_dict.items():\n",
    "        docs = 0\n",
    "        \n",
    "        # Loop through list of dicts.  Each time a dictionary contains the word, increment docs by 1\n",
    "        for d in list_of_dicts:\n",
    "            if word in d:\n",
    "                docs += 1\n",
    "        \n",
    "        # Now that we know denominator for equation, compute and set IDF value for word\n",
    "        \n",
    "        full_vocab_dict[word] = np.log((len(list_of_dicts)/ float(docs)))\n",
    "    \n",
    "    return full_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(list_of_dicts):\n",
    "    # Create empty dictionary containing full vocabulary of entire corpus\n",
    "    doc_tf_idf = {}\n",
    "    idf = inverse_document_frequency(list_of_dicts)\n",
    "    full_vocab_list = {i:0 for i in list(idf.keys())}\n",
    "    \n",
    "    # Create tf-idf list of dictionaries, containing a dictionary that will be updated for each document\n",
    "    tf_idf_list_of_dicts = []\n",
    "    \n",
    "    # Now, compute tf and then use this to compute and set tf-idf values for each document\n",
    "    for doc in list_of_dicts:\n",
    "        doc_tf = term_frequency(doc)\n",
    "        for word in doc_tf:\n",
    "            doc_tf_idf[word] = doc_tf[word] * idf[word]\n",
    "        tf_idf_list_of_dicts.append(doc_tf_idf)\n",
    "    \n",
    "    return tf_idf_list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(subreddits):\n",
    "    vectorized_dicts = []\n",
    "    for sub in subreddits:\n",
    "        # Clean and tokenize raw text\n",
    "        titles = all_submissions[sub]\n",
    "        tokenized = tokenize(titles)\n",
    "        \n",
    "        # Get count vectorized representation and store in vectorized_dicts  \n",
    "        count_vectorized_document = count_vectorize(tokenized)\n",
    "        vectorized_dicts.append(count_vectorized_document)\n",
    "    tf_idf_all_docs = tf_idf(vectorized_dicts)\n",
    "    return tf_idf_all_docs\n",
    "\n",
    "tf_idf_all_subs = main(clean_subs)\n",
    "print(list(tf_idf_all_subs[0])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = len(tf_idf_all_subs[0])\n",
    "print(\"Number of Dimensions: {}\".format(num_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vals_list = []\n",
    "\n",
    "for i in tf_idf_all_subs:\n",
    "    tf_idf_vals_list.append(list(i.values()))\n",
    "    \n",
    "tf_idf_vals_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "t_sne_object_3d = TSNE(n_components=3)\n",
    "transformed_data_3d = t_sne_object_3d.fit_transform(tf_idf_vals_list)\n",
    "transformed_data_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
