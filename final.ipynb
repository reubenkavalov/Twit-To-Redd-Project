{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Importing Required Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import praw\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec, CoherenceModel\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from keras.preprocessing import text\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import History, EarlyStopping, ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring User Likes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liked_tweets(handle):\n",
    "    '''\n",
    "    Takes Twitter handle and extracts and parses liked tweets.\n",
    "    \n",
    "    '''\n",
    "    ### Acquired from Twitter using API key and API secret key ###\n",
    "    bearer_token = 'AAAAAAAAAAAAAAAAAAAAAImyAwEAAAAAyWB9oAfMqhJG66%2BeJHuUEi5wsXM%3DKOzTDPgjqoXjzERO1JmoAJfVIqjnaPdD7wUhayNjESzeytty2J'\n",
    "    ###\n",
    "    all_tweets = []\n",
    "    url = 'https://api.twitter.com/1.1/favorites/list.json?screen_name={}'.format(handle)\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer {}'.format(bearer_token),\n",
    "            }\n",
    "    first_url_params = {\n",
    "        'count': 200\n",
    "            }\n",
    "    first_response = requests.get(url, headers=headers, params=first_url_params)\n",
    "    print(first_response)\n",
    "    first_initial_split = first_response.text.split(',\"text\":\"')\n",
    "    first_t = len(first_initial_split)    \n",
    "    for i in range(1,first_t):\n",
    "        all_tweets.append(first_initial_split[i].split(',\"truncated\":')[0])\n",
    "        \n",
    "    ### Regex to extract the id of the last tweet to use as pagination ###      \n",
    "    regsearch = re.search('(?<=(\\d\",\"id\":))[\\d]*', first_initial_split[first_t-2])\n",
    "    tweet_id = int(regsearch.group(0))\n",
    "    ###\n",
    "    \n",
    "    ### Looping process to manually paginate through tweets ###\n",
    "    page = 0\n",
    "    while page < 15:\n",
    "        url1 = 'https://api.twitter.com/1.1/favorites/list.json?screen_name={}&count=200&max_id={}'.format(handle,tweet_id-1)\n",
    "        response = requests.get(url1, headers=headers)\n",
    "        initial_split = response.text.split(',\"text\":\"')\n",
    "        t = len(initial_split)\n",
    "        for i in range(1,t):\n",
    "            all_tweets.append(initial_split[i].split(',\"truncated\":')[0])\n",
    " \n",
    "        ### Regex to extract the id of the last tweet to use as pagination ###      \n",
    "        try:\n",
    "            regsearch = re.search('(?<=(\\d\",\"id\":))[\\d]*', initial_split[t-2])\n",
    "            tweet_id = int(regsearch.group(0))\n",
    "        except: break\n",
    "        ###\n",
    "        \n",
    "        page +=1\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Top Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.reddit.com/r/TheoryOfReddit/comments/1f7hqc/the_200_most_active_subreddits_categorized_by/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = driver.find_elements_by_class_name('_1qeIAgB0cPwnLhDF9XSiJM')\n",
    "subreddits = []\n",
    "for i in subs[3:]:\n",
    "    if \"r/\" in i.text:\n",
    "        subreddits.append(i.text.split(\" - \")[1])\n",
    "    else:\n",
    "        continue\n",
    "### THIS CELL THROWS AN ERROR -- IGNORE IT -- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_subs = []\n",
    "for sub in subreddits:\n",
    "    clean_subs.append(sub.split('r/')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PRAW (Python Reddit API Wrapper) to get top posts in top subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='MU9yce7zmiOKwA',\n",
    "                     client_secret='DRV5fO7p-Tn7VzxmMH8mnRwM_hE',\n",
    "                     user_agent='my user agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = {}\n",
    "for sub in tqdm(clean_subs):\n",
    "    all_submissions[sub] = []\n",
    "    for submission in reddit.subreddit(sub).top(limit=1000):\n",
    "        all_submissions[sub].append(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at examples of pre-processing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "subr_text_tokens_raw = nltk.regexp_tokenize(\" \".join(all_submissions['creepy']), pattern)\n",
    "subr_text_tokens = [word.lower() for word in subr_text_tokens_raw]\n",
    "subr_text_tokens = [lemmatizer.lemmatize(i) for i in subr_text_tokens]\n",
    "subr_text_freqdist = FreqDist(subr_text_tokens)\n",
    "subr_text_freqdist.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_text_stopped_freqdist = FreqDist(subr_text_stopped)\n",
    "subr_text_stopped_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_word_count = sum(subr_text_stopped_freqdist.values())\n",
    "subr_text_top_50 = subr_text_stopped_freqdist.most_common(50)\n",
    "print(\"Word\\t\\t\\tNormalized Frequency\")\n",
    "for word in subr_text_top_50:\n",
    "    normalized_frequency = word[1] / total_word_count\n",
    "    print(\"{} \\t\\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "subr_text_finder = BigramCollocationFinder.from_words(subr_text_stopped)\n",
    "subr_text_finder.apply_freq_filter(3)\n",
    "\n",
    "subr_text_scored = subr_text_finder.score_ngrams(bigram_measures.pmi)\n",
    "subr_text_scored[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating subreddits into respective categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text_df = pd.DataFrame()\n",
    "label_text_df['target'] = ['Discussion and Stories','Emotional Reaction Fuel','Entertainment - Gaming', \n",
    "                         'Entertainment - Television', 'Entertainment - Other (Movies/Music/Franchies/Misc)',\n",
    "                         'Humor','Images, Gifs, and Videos','Learning and Thinking','Lifestyle and Help',\n",
    "                         'News and Issues', 'Places', 'Race, Gender, and Identity', 'Sports','Technology']\n",
    "categorized_posts = dict((i,\"\")for i in label_text_df.target)\n",
    "for i in clean_subs:\n",
    "    if i ==\"AskReddit\" or i=='IAmA' or i ==\"bestof\" or i=='fatpeoplestories' or i =='pettyrevenge' or i ==\"TalesFromRetail\" or i=='DoesAnybodyElse' or i =='CrazyIdeas':\n",
    "        categorized_posts['Discussion and Stories'] = [categorized_posts['Discussion and Stories'],all_submissions[i]]\n",
    "    elif i ==\"WTF\" or i=='aww' or i ==\"cringepics\" or i=='JusticePorn' or i =='MorbidReality' or i ==\"rage\" or i=='mildlyinfuriating' or i =='creepy' or i=='creepyPMs' or i ==\"nosleep\" or i=='nostalgia':\n",
    "        categorized_posts['Emotional Reaction Fuel'] = [categorized_posts['Emotional Reaction Fuel'],all_submissions[i]]\n",
    "    elif i ==\"gaming\" or i=='leagueoflegends' or i ==\"pokemon\" or i=='Minecraft' or i =='starcraft' or i ==\"Games\" or i=='DotA2' or i =='skyrim' or i=='tf2' or i ==\"magicTCG\" or i=='wow' or i ==\"KerbalSpaceProgram\" or i=='mindcrack' or i =='Fallout' or i=='roosterteeth' or i ==\"Planetside\" or i=='gamegrumps' or i ==\"battlefield3\" or i=='zelda' or i =='darksouls' or i=='masseffect':\n",
    "        categorized_posts['Entertainment - Gaming'] = [categorized_posts['Entertainment - Gaming'],all_submissions[i]]\n",
    "    elif i ==\"arresteddevelopment\" or i=='gameofthrones' or i ==\"doctorwho\" or i=='mylittlepony' or i =='community' or i ==\"breakingbad\" or i=='adventuretime' or i =='startrek' or i=='TheSimpsons' or i ==\"futurama\" or i=='HIMYM' or i=='DunderMifflin' or i ==\"thewalkingdead\":\n",
    "        categorized_posts['Entertainment - Television'] = [categorized_posts['Entertainment - Television'],all_submissions[i]]\n",
    "    elif i ==\"Music\" or i=='movies' or i ==\"harrypotter\" or i=='StarWars' or i =='DaftPunk' or i ==\"hiphopheads\" or i=='anime' or i =='comicbooks' or i=='geek' or i ==\"batman\" or i=='TheLastAirbender' or i=='Naruto' or i ==\"FanTheories\":\n",
    "        categorized_posts['Entertainment - Other (Movies/Music/Franchies/Misc)'] = [categorized_posts['Entertainment - Other (Movies/Music/Franchies/Misc)'],all_submissions[i]]\n",
    "    elif i ==\"funny\" or i=='AdviceAnimals' or i ==\"fffffffuuuuuuuuuuuu\" or i=='4chan' or i =='ImGoingToHellForThis' or i ==\"firstworldanarchists\" or i=='circlejerk' or i =='MURICA' or i=='facepalm' or i ==\"Jokes\" or i=='wheredidthesodago' or i=='polandball' or i ==\"TrollXChromosomes\" or i ==\"comics\" or i=='nottheonion' or i=='britishproblems' or i ==\"TumblrInAction\" or i ==\"onetruegod\":\n",
    "        categorized_posts['Humor'] = [categorized_posts['Humor'],all_submissions[i]]\n",
    "    elif i ==\"pics\" or i=='videos' or i ==\"gifs\" or i=='reactiongifs' or i =='mildlyinteresting' or i ==\"woahdude\" or i=='FiftyFifty' or i =='FoodPorn' or i=='HistoryPorn' or i ==\"wallpapers\" or i=='youtubehaiku' or i=='Unexpected' or i ==\"photoshopbattles\" or i ==\"AnimalsBeingJerks\" or i=='cosplay' or i=='EarthPorn' or i ==\"QuotesPorn\" or i ==\"awwnime\" or i ==\"AbandonedPorn\" or i=='carporn' or i ==\"PerfectTiming\" or i=='OldSchoolCool' or i =='RoomPorn' or i ==\"woahdude\" or i=='Pareidolia' or i =='MapPorn' or i=='tumblr' or i ==\"techsupportgore\" or i=='PrettyGirls' or i=='itookapicture':\n",
    "        categorized_posts['Images, Gifs, and Videos'] = [categorized_posts['Images, Gifs, and Videos'],all_submissions[i]]\n",
    "    elif i ==\"todayilearned\" or i=='science' or i ==\"askscience\" or i=='space' or i =='AskHistorians' or i ==\"YouShouldKnow\" or i=='explainlikeimfive':\n",
    "        categorized_posts['Learning and Thinking'] = [categorized_posts['Learning and Thinking'],all_submissions[i]]\n",
    "    elif i ==\"trees\" or i=='MakeupAddiction' or i ==\"cats\" or i=='LifeProTips' or i =='RedditLaqueristas' or i ==\"Random_Acts_Of_Amazon\" or i=='food' or i =='guns' or i=='tattoos' or i ==\"corgi\" or i=='teenagers' or i =='GetMotivated' or i=='motorcycles' or i ==\"sex\" or i=='progresspics' or i =='DIY' or i=='bicycling' or i ==\"Fitness\" or i=='lifehacks' or i =='longboarding' or i=='Frugal' or i ==\"drunk\" or i=='Art' or i=='loseit' or i =='Military':\n",
    "        categorized_posts['Lifestyle and Help'] = [categorized_posts['Lifestyle and Help'],all_submissions[i]]\n",
    "    elif i ==\"politics\" or i=='worldnews' or i ==\"news\" or i=='conspiracy' or i =='Libertarian' or i ==\"TrueReddit\" or i=='Conservative' or i=='offbeat':\n",
    "        categorized_posts['News and Issues'] = [categorized_posts['News and Issues'],all_submissions[i]]\n",
    "    elif i ==\"canada\" or i=='toronto' or i ==\"australia\" or i=='unitedkingdom':\n",
    "        categorized_posts['Places'] = [categorized_posts['Places'],all_submissions[i]]\n",
    "    elif i ==\"atheism\" or i=='TwoXChromosomes' or i ==\"MensRights\" or i=='gaybros' or i =='lgbt':\n",
    "        categorized_posts['Race, Gender, and Identity'] = [categorized_posts['Race, Gender, and Identity'],all_submissions[i]]\n",
    "    elif i ==\"nba\" or i=='soccer' or i ==\"hockey\" or i=='nfl' or i =='formula1' or i ==\"baseball\" or i=='MMA' or i=='SquaredCircle':\n",
    "        categorized_posts['Sports'] = [categorized_posts['Sports'],all_submissions[i]]\n",
    "    elif i ==\"technology\" or i=='Android' or i ==\"Bitcoin\" or i=='programming' or i =='apple':\n",
    "        categorized_posts['Technology'] = [categorized_posts['Technology'],all_submissions[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataframe of posts and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(container):\n",
    "    for i in container:\n",
    "        if isinstance(i, (list,tuple)):\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i\n",
    "posts_by_category = dict((i,\"\")for i in label_text_df.target)\n",
    "for i in label_text_df.target:\n",
    "    posts_by_category[i] = list(flatten(categorized_posts[i]))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = []\n",
    "for i in tqdm(label_text_df.target):\n",
    "    for post in posts_by_category[i]:\n",
    "        post_list = []\n",
    "        post_list.append(i)\n",
    "        post_list.append(post)\n",
    "        master_list.append(post_list)\n",
    "\n",
    "reddit_df = pd.DataFrame(master_list)\n",
    "reddit_df.to_csv('reddit_df') ### Saving dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('reddit_df') ### Loading dataframe\n",
    "reddit_df = reddit_df.drop(columns=[\"Unnamed: 0\"])\n",
    "reddit_df.columns = reddit_df.columns.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(reddit_df_stopped['tokens'])\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.66, keep_n=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn2numbers(tokens):\n",
    "    word_tokens = [dictionary.token2id[token] for token in tokens if token in dictionary.token2id]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df_stopped['filtered'] = reddit_df_stopped['tokens'].apply(lambda x: turn2numbers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GloVes multi-dimensional word vectorizer to build baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = reddit_df[0]\n",
    "data = reddit_df[1].map(word_tokenize).values\n",
    "\n",
    "total_vocabulary = set(word for headline in data for word in headline)\n",
    "print(\"There are {} unique tokens in the dataset.\".format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('glove.6B.300d.txt', 'rb') as f:\n",
    "    for line in f:                                    \n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])\n",
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]\n",
    "\n",
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in tqdm(models)]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial results seem low, but, with 14 categories, random guessing would achieve an accuracy of 7.1%. Let's try a Deep Neural Network with a Word Embedding Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values\n",
    "reddit_df['splitted'] = reddit_df[1].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "reddit_df['text_lemmatized'] = reddit_df.splitted.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reddit_df[1] = reddit_df.text_lemmatized.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(reddit_df[1]))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(reddit_df[1])\n",
    "X_t = pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#      BEST  MODEL      #  --- Epoch 1: 15.6% accurate\n",
    "#########################  --- Epoch 50: 49.4%\n",
    "\n",
    "embedding_size = 300\n",
    "input_ = Input(shape=(50,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = Dense(750, activation='relu', kernel_regularizer=regularizers.l2(0.2))(x)\n",
    "\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(500, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "# There are 14 different possible classes, so we use 14 neurons in our output layer\n",
    "x = Dense(14, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "X_train,X_val, y_train,y_val = train_test_split(X_t,y,test_size=0.1,random_state=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network initially kept having a val_accuracy of 0... because the data is sitting in order and wasn't shuffled! (the \"test_size\" is just the very last bit (0.2) of the data, it doesn't choose randomly... fixed with train test split, which shuffles by default**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failed - Training Neural Network with stopwords removed from post titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_speech_text(text):\n",
    "    return [w.lower() for w in nltk.word_tokenize(text)]\n",
    "reddit_df_stopped['tokens'] = reddit_df_stopped['full_text'].apply(lambda x: lemmatize_speech_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "reddit_df_stopped = pd.DataFrame()\n",
    "reddit_df_stopped[0] = reddit_df[0]\n",
    "reddit_df_stopped[1] = reddit_df[1].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "reddit_df_stopped['full_text'] = reddit_df_stopped[1].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "reddit_df_stopped['full_text'] = reddit_df_stopped['full_text'].apply(lambda x: re.sub(r'[\\d]', '', x))\n",
    "\n",
    "X_t = pad_sequences(reddit_df_stopped['filtered'], maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "input_ = Input(shape=(50,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = Dense(750, activation='relu', kernel_regularizer=regularizers.l2(0.2))(x)\n",
    "\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(500, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "# There are 14 different possible classes, so we use 14 neurons in our output layer\n",
    "x = Dense(14, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "X_train,X_val, y_train,y_val = train_test_split(X_t,y,test_size=0.3,random_state=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=500, callbacks=[earlyStopping,mcp_save], shuffle=True, validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success - Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reddit_df_stopped.full_text\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating probability of what category you like based on tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RedditCategoryPredictor():\n",
    "    twitter_handle = input(\"What is your Twitter Handle?\")\n",
    "    likes = get_liked_tweets(twitter_handle)\n",
    "    print(\"Grabbed {} likes\".format(len(likes)))     \n",
    "    \n",
    "    joined_words = []\n",
    "    for i in set(likes):\n",
    "        joined_words.append(\"\".join(i))\n",
    "    posts_df = pd.DataFrame(joined_words)\n",
    "    posts_df['full_text'] = posts_df[0].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))         ### Removing Unnecessary\n",
    "    posts_df['full_text'] = posts_df['full_text'].apply(lambda x: re.sub(r'[\\d]', '', x))  ### Characters \n",
    "    posts_df['final'] = posts_df['full_text'].apply(lambda x: x.split())\n",
    "\n",
    "    dictionary = corpora.Dictionary(posts_df['final'])\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.66, keep_n=20000)    \n",
    "    tokens =list(flatten(posts_df['final']))\n",
    "    new_vector = model.infer_vector(tokens)\n",
    "    sims = model.docvecs.most_similar([new_vector],topn=50)\n",
    "    final_results = []\n",
    "    for i in sims:\n",
    "        final_results.append(reddit_df_stopped.iloc[int(i[0])][0])\n",
    "    cnt = Counter()\n",
    "    for i in final_results:\n",
    "        cnt[i] += 1\n",
    "    print(max(cnt))\n",
    "    return max(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(reddit_df_stopped.tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=12, random_state=42,\n",
    "                                             workers=3, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Related Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring subreddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_subreddits = {}\n",
    "for i in clean_subs:\n",
    "    searchbox = driver.find_element_by_id('search-input')\n",
    "    searchbox.send_keys(i)\n",
    "    driver.find_element_by_class_name('btn').click()\n",
    "    result_subs = driver.find_elements_by_xpath(\"//div[contains(@ng-repeat, 'sub in to')]\")\n",
    "    for s in result_subs:\n",
    "        related_subreddits.setdefault(i,[]).append(s.text) \n",
    "    driver.find_element_by_id('search-input').clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "for related_subs in tqdm(list(related_subreddits.values())):\n",
    "    for sub in related_subs:\n",
    "        driver.get('https://www.reddit.com/r/{}'.format(sub))\n",
    "        try:\n",
    "            driver.find_element_by_class_name('_1jefpljVGT-eHObg40F8Dm')\n",
    "            related_subs.remove(sub)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_related_posts = {}\n",
    "for related_subs_key,related_subs_val in tqdm(list(related_subreddits.items())):\n",
    "    related_sub_posts = []\n",
    "    for sub in tqdm(related_subs_val):\n",
    "        new_str = \"\"\n",
    "        try:\n",
    "            for submission in reddit.subreddit(sub).top(limit=1000):\n",
    "                new_str = new_str + submission.title + \" \"\n",
    "        except:\n",
    "            continue\n",
    "        related_sub_posts.append((new_str,sub,related_subs_key))\n",
    "    all_related_posts[related_subs_key] = related_sub_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "related_subdf = {}\n",
    "for parent_sub in tqdm(list(related_subreddits.keys())):\n",
    "    for sub_tuple in all_related_posts[parent_sub]:\n",
    "        related_subdf[sub_tuple[1]] = sub_tuple[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_subdf = pd.DataFrame.from_dict(related_subdf,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_subdf.to_csv('related_subdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_subdf=pd.read_csv('related_subdf',index_col='Unnamed: 0')\n",
    "related_subdf.columns=related_subdf.columns.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_subdf['clean_text'] = related_subdf[0].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "related_subdf['clean_text'] = related_subdf['clean_text'].apply(lambda x: re.sub(r'[\\d]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = related_subdf.clean_text\n",
    "tagged_data2 = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model on data from related subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model_relatedsubs = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model_relatedsubs.build_vocab(tagged_data2)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model_relatedsubs.train(tagged_data2,\n",
    "                total_examples=model_relatedsubs.corpus_count,\n",
    "                epochs=model_relatedsubs.iter)\n",
    "    # decrease the learning rate\n",
    "    model_relatedsubs.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model_relatedsubs.min_alpha = model_relatedsubs.alpha\n",
    "\n",
    "model_relatedsubs.save(\"d2v_relatedsubs.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_subs_df = related_subdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_subs_df = indexed_subs_df.reset_index()\n",
    "indexed_subs_df.columns=['index_no','sub','0','clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_subs_df = indexed_subs_df.drop(columns=['0'])\n",
    "indexed_subs_df.to_csv('related_subs_and_indexes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding most similar subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_subreddit(sub_name):\n",
    "    sub_num = indexed_subs_df[indexed_subs_df['sub']==sub_name]['index_no'].values[0]\n",
    "    similar_subs = pd.DataFrame(model_relatedsubs.docvecs.most_similar(sub_num, topn=5))\n",
    "    similar_subs.columns = ['index_no', 'Percent Similarity']\n",
    "    similar_subs['index_no'] = similar_subs['index_no'].apply(lambda x: int(x))\n",
    "    similar_subs['Percent Similarity'] = similar_subs['Percent Similarity'].apply(lambda x: str(round(x*100))+'%')\n",
    "    similar_df = indexed_subs_df.merge(similar_subs, how='inner', on='index_no')[['sub','Percent Similarity']]\n",
    "    return similar_df.sort_values(by='Percent Similarity',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_speech_wordcloud(row):\n",
    "    mycmap = 'magma'\n",
    "    wordcloud = WordCloud(width=1600, height=800,# stopwords=stopwords_list,\n",
    "                        background_color='white', max_words=120,\n",
    "                        colormap=mycmap).generate(row['clean_text'])\n",
    "    filename = 'wordcloud' + str(row['index_no']) + '.png'\n",
    "    wordcloud.to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_subs_df.apply(lambda x: create_speech_wordcloud(x), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
